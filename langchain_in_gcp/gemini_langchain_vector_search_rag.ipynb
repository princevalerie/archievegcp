{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64a236-060b-4648-89fb-8ad86addde67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip3 install -q --upgrade pip\n",
    "!pip3 install -q google-cloud-aiplatform\n",
    "!pip3 install -q langchain\n",
    "!pip3 install -q langchain-community\n",
    "!pip3 install -q lxml\n",
    "!pip3 install -q requests\n",
    "!pip3 install -q beautifulsoup4\n",
    "!pip3 install -q unstructured\n",
    "!pip3 install -q langchain-google-genai\n",
    "!pip3 install -q google-generativeai\n",
    "!pip3 install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371c287-5c4d-4f6d-9483-787ddb7a79c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac7c23-8831-4594-a9aa-a1aade9da542",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f13fb6-6a06-4dcb-9812-4643cb1f9cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754a634-726f-4e04-bbec-ff241cfbdb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source API key from GCP project and configure genai client\n",
    "import os\n",
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "key_name = !gcloud services api-keys list --filter=\"gemini-api-key\" --format=\"value(name)\"\n",
    "key_name = key_name[0]\n",
    "\n",
    "api_key = !gcloud services api-keys get-key-string $key_name --location=\"us-central1\" --format=\"value(keyString)\"\n",
    "api_key = api_key[0]\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db35058-5ee4-4eda-94d1-1c8fa96235fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "PROJECT_ID = subprocess.check_output([\"gcloud\", \"config\", \"get-value\", \"project\"], text=True).strip()\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781195e-7ab6-455b-9397-e166352bfb09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment vars\n",
    "BUCKET = f\"gs://{PROJECT_ID}/embeddings\"\n",
    "DIMENSIONS=768\n",
    "DISPLAY_NAME='vertex_docs_qa'\n",
    "ENDPOINT=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "TEXT_GENERATION_MODEL='gemini-pro'\n",
    "SITEMAP='https://docs.anthropic.com/sitemap.xml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d201b-404b-4fbb-8ff2-7e45096ab2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f243a82-fb52-4874-a921-978a5a2769c4",
   "metadata": {},
   "source": [
    "# Task 1: Create Documents from Vertex AI Cloud Documentation Site\n",
    "\n",
    "## Load and parse sitemap.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c90bd-1d13-4b5b-baaa-9d17b6097b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the xml of sitemap and get URLs of doc site\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_sitemap(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    urls = [element.text for element in soup.find_all(\"loc\")]\n",
    "    return urls\n",
    "\n",
    "sites = parse_sitemap(SITEMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375b356-263c-481a-89eb-19eeacbf2088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this to filter out docs that don't have a corresponding reference page\n",
    "sites_filtered = [url for url in sites if '/en/docs' in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956153-9c15-42b2-83fc-6c30c13d21c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(sites_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc151f1f-0416-4421-909d-b792750220d2",
   "metadata": {},
   "source": [
    "## Load documentation pages using the LangChain UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82632343-60bd-4c3c-a39d-ae728f75d1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This step will take a few minutes to complete\n",
    "# you will see download messages below the cell after execution\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "loader = UnstructuredURLLoader(urls=sites_filtered)\n",
    "documents = loader.load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537a9e4-85e1-44c8-9c91-fd32bc527566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_markdown(documents[1].page_content + \"\\n\\nSource: \" + documents[1].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb3e49-cd62-4043-9440-5013eb1797b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f00b5-4022-467e-a001-008b8a53768c",
   "metadata": {},
   "source": [
    "## Create Document chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccee30-c90b-4243-a445-53950364b3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# recursively loop through the text and create document chunks for embedding\n",
    "import warnings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #separator = \"\\n\",\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 100)\n",
    "\n",
    "document_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number documents {len(documents)}\")\n",
    "print(f\"Number chunks {len(document_chunks)}\")\n",
    "\n",
    "document_chunks=[f\"content: {chunk.page_content}, source: {chunk.metadata['source']}\" for chunk in document_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7387e-11df-4d72-9fb0-dda3acc098d8",
   "metadata": {},
   "source": [
    "# Task 2: Generate embeddings from Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57362bd-7e38-445c-ada9-20e57132bbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a documents directory\n",
    "!rm -rf ./documents\n",
    "!mkdir ./documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6e60a-aa97-4553-a50d-58649e84df60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the document chunks in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(document_chunks, columns =['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0a5ff-26a5-4023-b833-fb9f9b418a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to generate the embeddings files you will later upload to Cloud Storage\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "index_embeddings = []\n",
    "model = \"models/embedding-001\"\n",
    "\n",
    "for index, doc in tqdm(df.iterrows(), total=len(df), position=0):\n",
    "\n",
    "    response = genai.embed_content(model=model, content=doc['text'], task_type=\"retrieval_query\")\n",
    "\n",
    "    doc_id=f\"{index}.txt\"\n",
    "    embedding_dict = {\n",
    "        \"id\": doc_id,\n",
    "        \"embedding\": response[\"embedding\"],\n",
    "    }\n",
    "    index_embeddings.append(json.dumps(embedding_dict) + \"\\n\")\n",
    "    \n",
    "    with open(f\"documents/{doc_id}\", \"w\") as document:\n",
    "          document.write(doc['text'])\n",
    "    \n",
    "with open(\"embeddings.json\", \"w\") as f:\n",
    "    f.writelines(index_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93913e92-4c3f-46cc-9498-cde46fe559f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "source_file = '/home/jupyter/embeddings.json'\n",
    "destination_blob_name = 'embeddings/embeddings.json' # Adjust if needed\n",
    "\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.bucket(PROJECT_ID)\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "blob.upload_from_filename(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c1278-41f3-4892-bd7d-a9d6eb7848c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the embedding files to Cloud Storage\n",
    "# This step will take a few minutes to complete\n",
    "import subprocess\n",
    "gsutil_command = f\"gsutil -q cp -r './documents' gs://{PROJECT_ID}/documents\"\n",
    "\n",
    "subprocess.run(['gsutil', '-q', 'cp', '-r', './documents', f'gs://{PROJECT_ID}/documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056208e-7570-4005-b274-90e62d539fb9",
   "metadata": {},
   "source": [
    "# Task 3. Create a Vertex AI Vector Store index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566d26d-d5eb-4774-80a3-dc61b570fd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Vertex AI Vector Search index\n",
    "# This step will take several minutes to complete\n",
    "# Wait for this cell to complete before proceeding\n",
    "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "      display_name=\"vertex_docs\",\n",
    "      contents_delta_uri=f\"gs://{PROJECT_ID}/embeddings\",\n",
    "      dimensions=768,\n",
    "      approximate_neighbors_count=150,\n",
    "      distance_measure_type=\"DOT_PRODUCT_DISTANCE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced8d73-3b98-4e55-89ca-a29bcb086d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"vertex_docs\",\n",
    "    description=\"Embeddings for the documentation curated from the sitemap.\",\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d4077-275f-43cf-8c55-993e0880d651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This step will take up to 20 minutes to complete\n",
    "# You can view the deployment in the Vertex AI console on the \"Vector Search\" tab\n",
    "# Wait for this cell to complete before proceeding\n",
    "index_endpoint = index_endpoint.deploy_index(\n",
    "    index=index, deployed_index_id=\"vertex_index_deployment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877608b4-9496-48c5-8513-799d6324f3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME=index.resource_name\n",
    "index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)\n",
    "\n",
    "deployed_index = index.deployed_indexes\n",
    "deployed_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a81baa-a4f6-4712-8db2-339b3ca74e35",
   "metadata": {},
   "source": [
    "# Task 4: Search Vector Store, add result as context to a query (without using a LangChain Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f563bb-4417-4eac-b675-2377e151357d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In the next cells you will query the model directly using the Vertex AI python SDK\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores.matching_engine import MatchingEngine\n",
    "from langchain.agents import Tool\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "def search_vector_store(question):\n",
    "\n",
    "    vector_store = MatchingEngine.from_components(\n",
    "                        index_id=INDEX_RESOURCE_NAME,\n",
    "                        region=REGION,\n",
    "                        embedding=embeddings,\n",
    "                        project_id=PROJECT_ID,\n",
    "                        endpoint_id=deployed_index[0].index_endpoint,\n",
    "                        gcs_bucket_name=f\"{PROJECT_ID}\")\n",
    "    \n",
    "    relevant_documentation=vector_store.similarity_search(question, k=8)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_documentation])[:10000]\n",
    "    return str(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42442602-f577-4cb2-b4a9-e26fa2425cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "import warnings\n",
    "\n",
    "# filter warnings for unused libs\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ask_question(question):\n",
    "    context = search_vector_store(question)\n",
    "\n",
    "    prompt=f\"\"\"\n",
    "        Follow exactly those 3 steps:\n",
    "        1. Read the context below and aggregrate this data\n",
    "        Context : {context}\n",
    "        2. Answer the question using only this context\n",
    "        3. Show the source for your answers\n",
    "        User Question: {question}\n",
    "\n",
    "\n",
    "        If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "        \"\"\"\n",
    "\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    return to_markdown(f\"Question: \\n{question} \\n\\n Response: \\n {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61376c0e-0447-4527-b212-e65f6b128fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"How do I reduce prompt leaks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef10e5-28ab-49f1-bc35-a783ef03e572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"What use cases and capabilities does Anthropic support?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730748e8-587d-4a07-b707-9f945e6ec96a",
   "metadata": {},
   "source": [
    "# Task 5: Create Retrieval Augmentation Generation application using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0e958-9d7b-44f0-8893-a508250fa836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To answer questions and chain together the prompt, vector search, returned context and model input use a LangChain \"Chain\"\n",
    "# In this case you will use the RetrievalQA chain which is commonly used for Question/Answering applications\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# initialize model using chat\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.0, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d89b2-7cab-4b72-ab2c-c0c931c777ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    Follow exactly those 3 steps:\n",
    "    1. Read the context below and aggregrate this data\n",
    "    Context : {context}\n",
    "    \n",
    "    2. Answer the question using only this context\n",
    "    3. Show the source for your answers\n",
    "    User Question: {question}\n",
    "\n",
    "    If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\",  \"question\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576213f-c4f0-4603-9ffb-5f4692f425e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.matching_engine import MatchingEngine\n",
    "\n",
    "vector_store = MatchingEngine.from_components(\n",
    "    index_id=INDEX_RESOURCE_NAME,\n",
    "    region=REGION,\n",
    "    embedding=embeddings,\n",
    "    project_id=PROJECT_ID,\n",
    "    endpoint_id=deployed_index[0].index_endpoint,\n",
    "    gcs_bucket_name=f\"{PROJECT_ID}\"\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 1}\n",
    ")\n",
    "\n",
    "# Test the retriever with a simple search performed above\n",
    "to_markdown(retriever.get_relevant_documents(\"How do I get started with Anthropic?\")[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa14bf-0cab-4955-8fb2-d44682c52756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756d95c-fe7f-4eaa-b1c7-0fa6f3fb5187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    response = qa({\"query\": question})\n",
    "\n",
    "    # since k is set to 1 only return the first source retrieved\n",
    "    source = response['source_documents']\n",
    "    \n",
    "    return to_markdown(f\"Response: \\n\\n {response['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220d283-bf02-4a4b-9975-dc3d0a4c2b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: You will see a library warning when running this step\n",
    "ask_question(\"How do I get started with Anthropic?\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-15.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-15:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
