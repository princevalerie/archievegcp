{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5d726-4dd0-4158-bee0-f0abaa1bd6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install langchain_experimental langchain_core\n",
    "!pip -q install google-generativeai==0.3.1\n",
    "!pip -q install google-ai-generativelanguage==0.4.0\n",
    "!pip -q install langchain-google-genai\n",
    "!pip -q install wikipedia\n",
    "!pip -q install docarray\n",
    "!pip -q install --upgrade protobuf google.protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before proceeding with the next cells restart the kernel by clicking the refresh icon on the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ae5e6-7580-4d67-b7fd-3a875b8cf649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from google.protobuf.empty_pb2 import Empty\n",
    "\n",
    "key_name = !gcloud services api-keys list --filter=\"gemini-api-key\" --format=\"value(name)\"\n",
    "key_name = key_name[0]\n",
    "\n",
    "api_key = !gcloud services api-keys get-key-string $key_name --location=\"us-central1\" --format=\"value(keyString)\"\n",
    "api_key = api_key[0]\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538d742-6944-4041-9dde-073385a87b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [m for m in genai.list_models()]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36455709-7797-4a28-a9a9-89a038e96b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Gemini directly with Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08290d-120a-489e-a4e3-46bfbd619bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate text\n",
    "prompt = 'Who are you and what can you do?'\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "Markdown(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214aef1-1aad-4469-a216-9d4a9b2cb278",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Gemini with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7f1ae-13f2-4a28-bd09-237eacea1106",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52da8e-0cee-409b-8a37-5c2ddca42aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)\n",
    "\n",
    "\n",
    "result = llm.invoke(\"What is a LLM?\")\n",
    "\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f5831-31e8-429a-9a79-e26fb096f9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chunk in llm.stream(\"Write a haiku about LLMs.\"):\n",
    "    print(chunk.content)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1410317-db72-4c2a-99dc-279938a07074",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Multi Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29cd459-e106-4407-8561-253d8b55ccf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53795e-8b12-4166-82a9-e242f6cf80c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc1c15-a8b9-48fc-a7cc-3cb9666fb822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fb13f-b6b1-407e-a4e0-16d16e31f636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"machine learning\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0802e31-78d7-42fa-a830-7a5daeed7872",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A more complicated Chain - RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086e684-8311-484a-84c4-d58a415c3c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e30de0-7dda-4997-9c54-c01d028e1c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2d34a-2a31-48c5-9ce1-4940a731b340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39116cea-1261-4eeb-a69a-9acc5b0104b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "# use Wikipedia loader to create some docs to use..\n",
    "docs = WikipediaLoader(query=\"Machine Learning\", load_max_docs=10).load()\n",
    "docs += WikipediaLoader(query=\"Deep Learning\", load_max_docs=10).load() \n",
    "docs += WikipediaLoader(query=\"Neural Networks\", load_max_docs=10).load() \n",
    "\n",
    "# Take a look at a single document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bfe25-419d-4caa-80d0-4878fb67cdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings # passing in the model to embed documents..\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2451a12-1e00-414a-a187-ab9675edb752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec6628-e399-4d0a-bc71-b74eac511648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what is gemini pro?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773eb751-451d-429a-ae3e-75287baea2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question a a full sentence, based only on the following context:\n",
    "{context}\n",
    "\n",
    "Return you answer in three back ticks\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a5efb-90ac-4b51-b323-cd5dceb95984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53ade5-9662-4f5a-a033-99622809163f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"What is a graident boosted tree?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec2b44-1798-4695-936c-7026b00117d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7b8f4-5426-4e87-8ac4-b58159cfa4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"What is machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751c12f-922e-4d75-8ce4-8533dc0c024b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"When was the transformer invented?\"})"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-13.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-13:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
